# ‚ö°Ô∏è NanoGPT "From Scratch" sur Apple MLX

Impl√©mentation d'un Transformer de type GPT-2 (Decoder-only) utilisant le framework **MLX**.
L'objectif de ce projet est de comprendre l'architecture interne des LLM et d'exploiter l'acc√©l√©ration mat√©rielle des puces Apple Silicon (M1/M2/M3/M4/M5) via l'API Metal, sans passer par la lourdeur de PyTorch/CUDA.

## üõ† Stack Technique & Pr√©requis
* **Langage :** Python 3.x
* **Framework :** Apple MLX (pour l'acc√®s bas niveau √† la m√©moire unifi√©e).
* **Architecture :** GPT-2 (Small config).
* **Tokenization :** Character-level (pour la simplicit√© d'impl√©mentation).

---

## 1. Pourquoi MLX ? (Sp√©cificit√©s Hardware)

Contrairement √† une approche classique o√π l'on g√®re manuellement les transferts CPU $\leftrightarrow$ GPU (`.to(device)`), MLX repose sur l'architecture **Unified Memory** d'Apple.

Dans mon code (`Phase 0`), on remarque deux concepts cl√©s :
1.  **Lazy Evaluation :** Les op√©rations comme `c = a + b` ne sont pas calcul√©es imm√©diatement. MLX construit un graphe de calcul et ne l'ex√©cute que lors de l'appel explicite √† `mx.eval()`. Cela permet au compilateur d'optimiser les kernels Metal.
2.  **Unified Arrays :** Les `mx.array` r√©sident dans la m√™me m√©moire adressable par le CPU et le GPU. Z√©ro copie = latence minimale.

---

## 2. Architecture du Mod√®le

Le mod√®le est un **Transformer Decoder-only** classique. Il prend une s√©quence d'indices en entr√©e et pr√©dit le prochain token.

### Flux de donn√©es (Dataflow)

```mermaid
graph TD
    Input[Indices Input] --> Emb[Embeddings Layer]
    Emb --> Block1[Transformer Block 1]
    Block1 --> Block2[Transformer Block 2]
    Block2 --> BlockN[... Block N]
    BlockN --> Norm[LayerNorm Final]
    Norm --> Head[Language Model Head]
    Head --> Logits[Logits de sortie]

```

---

## 3. Impl√©mentation D√©taill√©e

### 3.1 Embeddings (`Phase 5`)

Le r√©seau de neurones ne traite pas des strings, mais des vecteurs continus.

* **Token Embeddings (`wte`)** : Look-up table qui associe chaque caract√®re (ex: 'a') √† un vecteur dense de dimension 128 (`n_embd`).
* **Position Embeddings (`wpe`)** : Indispensable car l'attention est invariante par permutation. On ajoute un vecteur repr√©sentant la position  (0, 1, 2...) au vecteur du token.
* **R√®gle :** `x = wte(idx) + wpe(pos)`

### 3.2 Self-Attention : Le m√©canisme cl√© (`Phase 2`)

C'est ici que la "magie" contextuelle op√®re. Contrairement √† un RNN qui lit s√©quentiellement, l'attention permet √† chaque token de "regarder" tous les tokens pr√©c√©dents pour enrichir sa repr√©sentation.

J'ai impl√©ment√© l'attention causale (masked) manuellement pour la compr√©hension :

1. **Projections :** L'entr√©e  est projet√©e en 3 matrices : Query (), Key (), et Value ().
2. **Score d'attention :** On calcule le produit scalaire entre  et . Plus le score est haut, plus les tokens sont li√©s.
3. **Scaling & Masque :**
* Division par  pour stabiliser les gradients.
* Application d'un masque triangulaire inf√©rieur () pour emp√™cher le mod√®le de tricher en regardant les mots futurs.


4. **Agr√©gation :** Somme pond√©r√©e des Values ().

**La formule impl√©ment√©e :**

```mermaid
graph LR
    Input --> Proj[Linear Projection]
    Proj --> Split[Split Q, K, V]
    Split --> MatMul1[Q @ K.T]
    MatMul1 --> Scale[Scale & Mask]
    Scale --> Softmax
    Softmax --> MatMul2[Attn @ V]
    MatMul2 --> Output

```

### 3.3 MLP : Feed Forward Network (`Phase 3`)

Apr√®s l'agr√©gation d'information par l'attention, chaque token passe par un r√©seau dense ind√©pendant.

* Expansion :  (`4 * n_embd`).
* Activation : **GELU** (Gaussian Error Linear Unit), standard dans les LLM modernes (plus lisse que ReLU).
* Projection : .

### 3.4 Le Bloc Transformer & R√©sidus (`Phase 4`)

L'assemblage final utilise des **connexions r√©siduelles** (Skip connections).
Code : `x = x + self.attn(self.ln_1(x))`

Cela permet au gradient de fluer sans encombre lors de la backpropagation ("Gradient Superhighway"), r√©solvant le probl√®me de *vanishing gradient* sur les r√©seaux profonds. Notez l'utilisation de **Pre-Norm** (LayerNorm appliqu√© *avant* l'attention/MLP), standard depuis GPT-2 pour la stabilit√©.

---

## 4. Inf√©rence (G√©n√©ration)

La m√©thode `generate` (`Phase 6`) ex√©cute le mod√®le de mani√®re **auto-r√©gressive** :

1. Le mod√®le prend le contexte (ex: "Hell").
2. Il sort des logits pour le prochain caract√®re.
3. On applique un `argmax` (Greedy decoding) pour obtenir l'indice max (ex: "o").
4. On concat√®ne "o" √† l'entr√©e et on boucle.

*Optimisation :* Le code g√®re le d√©coupage du contexte (`idx_cond = idx[:, -block_size:]`) pour ne jamais d√©passer la taille maximale g√©r√©e par les embeddings de position.

---

## 5. Bibliographie & Ressources

Pour r√©aliser cette impl√©mentation, je me suis appuy√© sur les ressources suivantes :

* **Papier original Transformer :**
* Vaswani et al., *"Attention Is All You Need"* (2017). [ArXiv](https://arxiv.org/abs/1706.03762). *La base th√©orique.*


* **Architecture GPT-2 :**
* Radford et al., *"Language Models are Unsupervised Multitask Learners"* (OpenAI). [PDF](https://www.google.com/search?q=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). *Pour les d√©tails sur LayerNorm et l'initialisation.*


* **Impl√©mentation de r√©f√©rence :**
* Andrej Karpathy, *"Let's build GPT: from scratch, in code, spelled out."* [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY). *Ressource inestimable pour comprendre le passage de la th√©orie au code Python.*


* **Documentation Framework :**
* Apple Machine Learning Research, *MLX Documentation*. [GitHub](https://github.com/ml-explore/mlx). *Pour les sp√©cificit√©s de l'API `mlx.core` et `mlx.nn`.*